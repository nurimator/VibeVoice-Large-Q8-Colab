{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3568d95c",
   "metadata": {},
   "source": [
    "# VibeVoice Standalone Inference - Google Colab\n",
    "\n",
    "This is a standalone notebook to run VibeVoice TTS with ipywidgets interface.\n",
    "\n",
    "**⚠️ IMPORTANT: GPU Required**\n",
    "Before running this notebook, make sure to enable GPU:\n",
    "1. Go to `Runtime` → `Change runtime type`\n",
    "2. Select `Hardware accelerator`: **GPU**\n",
    "3. Select `GPU type`: **T4** (recommended)\n",
    "4. Click `Save`\n",
    "\n",
    "**Model**: FabioSarracino/VibeVoice-Large-Q8 (8-bit quantized)\n",
    "\n",
    "**Features**:\n",
    "- Single speaker and multi-speaker support\n",
    "- Voice cloning with audio upload\n",
    "- Interactive ipywidgets interface\n",
    "- Support for pause tags: `[pause]` or `[pause:1500]`\n",
    "- Automatic chunking for long texts\n",
    "\n",
    "---\n",
    "\n",
    "## Examples\n",
    "\n",
    "### Single Speaker Example\n",
    "Upload a voice for Speaker 1 only (or leave empty), then use:\n",
    "```\n",
    "Hello, this is a test. [pause:500] Let me continue speaking. [pause] And here's the final part.\n",
    "```\n",
    "\n",
    "### Multi Speaker Example\n",
    "Upload different voices for Speaker 1 and Speaker 2, then use:\n",
    "```\n",
    "[1]: Hello, how are you today?\n",
    "[2]: I'm doing great, thanks for asking!\n",
    "[1]: That's wonderful to hear. [pause:1000] What are your plans for the weekend?\n",
    "[2]: I'm planning to relax and maybe watch some movies.\n",
    "```\n",
    "\n",
    "### Notes\n",
    "- Upload voice files for each speaker you want to use\n",
    "- If only Speaker 1 has a voice uploaded, it will use Single Speaker mode\n",
    "- If 2 or more speakers have voices uploaded, it will use Multi Speaker mode\n",
    "- Use [1]:, [2]:, [3]:, [4]: format for multi-speaker text\n",
    "- You can use [pause] or [pause:milliseconds] for pauses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177eedcf",
   "metadata": {},
   "source": [
    "## 1. Setup and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66785057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q accelerate>=1.6.0 transformers>=4.51.3 diffusers tqdm scipy ml-collections\n",
    "!pip install -q torch>=2.0.0 torchaudio>=2.0.0 numpy>=1.20.0 librosa>=0.9.0 soundfile>=0.12.0\n",
    "!pip install -q peft>=0.17.0 huggingface_hub>=0.25.1 bitsandbytes\n",
    "!pip install -q ipywidgets\n",
    "\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236694f",
   "metadata": {},
   "source": [
    "## 2. Clone Repository and Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbdd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Clone repository from HuggingFace Spaces if not exists\n",
    "if not os.path.exists('/content/VibeVoice-Large-Q8-Colab'):\n",
    "    !git clone https://github.com/nurimator/VibeVoice-Large-Q8-Colab.git /content/VibeVoice-Large-Q8-Colab\n",
    "    print(\"Repository cloned from GitHub!\")\n",
    "else:\n",
    "    print(\"Repository already exists!\")\n",
    "\n",
    "# Add vvembed to path\n",
    "vvembed_path = '/content/VibeVoice-Large-Q8-Colab/vvembed'\n",
    "if vvembed_path not in sys.path:\n",
    "    sys.path.insert(0, vvembed_path)\n",
    "    print(f\"Added {vvembed_path} to Python path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d38b30",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257574db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from typing import List, Tuple, Optional\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio, HTML, clear_output\n",
    "from google.colab import files\n",
    "\n",
    "# Import VibeVoice components\n",
    "from modular.modeling_vibevoice_inference import VibeVoiceForConditionalGenerationInference\n",
    "from processor.vibevoice_processor import VibeVoiceProcessor\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb116355",
   "metadata": {},
   "source": [
    "## 4. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_voice_sample(speaker_idx: int, sample_rate: int = 24000) -> np.ndarray:\n",
    "    \"\"\"Create synthetic voice sample for a specific speaker\"\"\"\n",
    "    duration = 1.0\n",
    "    samples = int(sample_rate * duration)\n",
    "    t = np.linspace(0, duration, samples, False)\n",
    "\n",
    "    # Create realistic voice-like characteristics for each speaker\n",
    "    base_frequencies = [120, 180, 140, 200]  # Mix of male/female-like frequencies\n",
    "    base_freq = base_frequencies[speaker_idx % len(base_frequencies)]\n",
    "\n",
    "    # Create vowel-like formants (like \"ah\" sound) - unique per speaker\n",
    "    formant1 = 800 + speaker_idx * 100  # First formant\n",
    "    formant2 = 1200 + speaker_idx * 150  # Second formant\n",
    "\n",
    "    # Generate more voice-like waveform\n",
    "    voice_sample = (\n",
    "        # Fundamental with harmonics (voice-like)\n",
    "        0.6 * np.sin(2 * np.pi * base_freq * t) +\n",
    "        0.25 * np.sin(2 * np.pi * base_freq * 2 * t) +\n",
    "        0.15 * np.sin(2 * np.pi * base_freq * 3 * t) +\n",
    "\n",
    "        # Formant resonances (vowel-like characteristics)\n",
    "        0.1 * np.sin(2 * np.pi * formant1 * t) * np.exp(-t * 2) +\n",
    "        0.05 * np.sin(2 * np.pi * formant2 * t) * np.exp(-t * 3) +\n",
    "\n",
    "        # Natural breath noise (reduced)\n",
    "        0.02 * np.random.normal(0, 1, len(t))\n",
    "    )\n",
    "\n",
    "    # Add natural envelope (like human speech pattern)\n",
    "    vibrato_freq = 4 + speaker_idx * 0.3  # Slightly different vibrato per speaker\n",
    "    envelope = (np.exp(-t * 0.3) * (1 + 0.1 * np.sin(2 * np.pi * vibrato_freq * t)))\n",
    "    voice_sample *= envelope * 0.08  # Lower volume\n",
    "\n",
    "    return voice_sample.astype(np.float32)\n",
    "\n",
    "\n",
    "def load_audio_file(file_path: str, target_sr: int = 24000) -> np.ndarray:\n",
    "    \"\"\"Load audio file and resample to target sample rate\"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if audio.ndim > 1:\n",
    "        audio = librosa.to_mono(audio)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != target_sr:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "\n",
    "    # Normalize\n",
    "    audio_max = np.abs(audio).max()\n",
    "    if audio_max > 0:\n",
    "        audio = audio / max(audio_max, 1.0)\n",
    "\n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "\n",
    "def parse_pause_keywords(text: str) -> List[Tuple[str, any]]:\n",
    "    \"\"\"Parse [pause] and [pause:ms] keywords from text\"\"\"\n",
    "    segments = []\n",
    "    pattern = r'\\[pause(?::(\\d+))?\\]'\n",
    "\n",
    "    last_end = 0\n",
    "    for match in re.finditer(pattern, text):\n",
    "        # Add text segment before pause (if any)\n",
    "        if match.start() > last_end:\n",
    "            text_segment = text[last_end:match.start()].strip()\n",
    "            if text_segment:\n",
    "                segments.append(('text', text_segment))\n",
    "\n",
    "        # Add pause segment with duration (default 1000ms = 1 second)\n",
    "        duration_ms = int(match.group(1)) if match.group(1) else 1000\n",
    "        segments.append(('pause', duration_ms))\n",
    "        last_end = match.end()\n",
    "\n",
    "    # Add remaining text after last pause (if any)\n",
    "    if last_end < len(text):\n",
    "        remaining_text = text[last_end:].strip()\n",
    "        if remaining_text:\n",
    "            segments.append(('text', remaining_text))\n",
    "\n",
    "    # If no pauses found, return original text as single segment\n",
    "    if not segments:\n",
    "        segments.append(('text', text))\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "def generate_silence(duration_ms: int, sample_rate: int = 24000) -> np.ndarray:\n",
    "    \"\"\"Generate silence for specified duration\"\"\"\n",
    "    num_samples = int(sample_rate * duration_ms / 1000.0)\n",
    "    return np.zeros(num_samples, dtype=np.float32)\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text: str, max_words: int = 250) -> List[str]:\n",
    "    \"\"\"Split long text into manageable chunks at sentence boundaries\"\"\"\n",
    "    sentence_pattern = r'(?<=[.!?])\\s+(?=[A-Z])'\n",
    "    sentences = re.split(sentence_pattern, text)\n",
    "\n",
    "    if len(sentences) == 1 and len(text.split()) > max_words:\n",
    "        sentences = text.replace('. ', '.|').split('|')\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "\n",
    "        sentence_words = sentence.split()\n",
    "        sentence_word_count = len(sentence_words)\n",
    "\n",
    "        if sentence_word_count > max_words:\n",
    "            sub_parts = re.split(r'[,;]', sentence)\n",
    "            for part in sub_parts:\n",
    "                part = part.strip()\n",
    "                if not part:\n",
    "                    continue\n",
    "                part_words = part.split()\n",
    "                part_word_count = len(part_words)\n",
    "\n",
    "                if current_word_count + part_word_count > max_words and current_chunk:\n",
    "                    chunks.append(' '.join(current_chunk))\n",
    "                    current_chunk = [part]\n",
    "                    current_word_count = part_word_count\n",
    "                else:\n",
    "                    current_chunk.append(part)\n",
    "                    current_word_count += part_word_count\n",
    "        else:\n",
    "            if current_word_count + sentence_word_count > max_words and current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_word_count = sentence_word_count\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_word_count += sentence_word_count\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    if not chunks:\n",
    "        chunks = [text]\n",
    "\n",
    "    print(f\"Split text into {len(chunks)} chunks (max {max_words} words each)\")\n",
    "    return chunks\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262edaa",
   "metadata": {},
   "source": [
    "## 5. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = 'Quant-8Bit'\n",
    "MODEL_PATH = 'FabioSarracino/VibeVoice-Large-Q8'\n",
    "MODEL_URL = 'https://huggingface.co/FabioSarracino/VibeVoice-Large-Q8'\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(f\"Model path: {MODEL_PATH}\")\n",
    "print(f\"Model URL: {MODEL_URL}\")\n",
    "print(\"\\nThis may take a few minutes on first run...\\n\")\n",
    "\n",
    "# Check CUDA availability\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"Quantized models require a CUDA GPU. Please enable GPU in Colab Runtime settings.\")\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Configure 8-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model with 8-bit quantization\n",
    "model = VibeVoiceForConditionalGenerationInference.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Load processor\n",
    "processor = VibeVoiceProcessor.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Processor loaded successfully!\")\n",
    "print(\"\\nSetup complete! Ready to generate speech.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ca20d",
   "metadata": {},
   "source": [
    "## 6. Generate Speech Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio_segment(\n",
    "    formatted_text: str,\n",
    "    voice_samples: List[np.ndarray],\n",
    "    cfg_scale: float,\n",
    "    seed: int,\n",
    "    use_sampling: bool,\n",
    "    temperature: float,\n",
    "    top_p: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate audio for a single text segment\"\"\"\n",
    "\n",
    "    # Prepare inputs\n",
    "    inputs = processor(\n",
    "        [formatted_text],\n",
    "        voice_samples=[voice_samples],\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Move to device\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        if use_sampling:\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                tokenizer=processor.tokenizer,\n",
    "                cfg_scale=cfg_scale,\n",
    "                max_new_tokens=None,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "        else:\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                tokenizer=processor.tokenizer,\n",
    "                cfg_scale=cfg_scale,\n",
    "                max_new_tokens=None,\n",
    "                do_sample=False,\n",
    "            )\n",
    "\n",
    "    # Extract audio\n",
    "    if hasattr(output, 'speech_outputs') and output.speech_outputs:\n",
    "        speech_tensors = output.speech_outputs\n",
    "\n",
    "        if isinstance(speech_tensors, list) and len(speech_tensors) > 0:\n",
    "            audio_tensor = torch.cat(speech_tensors, dim=-1)\n",
    "        else:\n",
    "            audio_tensor = speech_tensors\n",
    "\n",
    "        # Convert to numpy\n",
    "        audio_np = audio_tensor.cpu().float().numpy()\n",
    "\n",
    "        # Flatten if needed\n",
    "        if audio_np.ndim > 1:\n",
    "            audio_np = audio_np.flatten()\n",
    "\n",
    "        return audio_np\n",
    "    else:\n",
    "        raise Exception(\"VibeVoice failed to generate audio\")\n",
    "\n",
    "print(\"Generation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dffa28",
   "metadata": {},
   "source": [
    "## 7. Create Interactive Interface with ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables to store uploaded audio files\n",
    "uploaded_audio_files = {\n",
    "    1: None,\n",
    "    2: None,\n",
    "    3: None,\n",
    "    4: None\n",
    "}\n",
    "\n",
    "# Create widgets\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Text input\n",
    "text_input = widgets.Textarea(\n",
    "    value='Hello, this is a test of the VibeVoice text-to-speech system.',\n",
    "    placeholder='Enter text here...',\n",
    "    description='Text:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='95%', height='120px')\n",
    ")\n",
    "\n",
    "# Upload buttons for voice samples\n",
    "upload_btn_1 = widgets.Button(description='Upload Speaker 1 Voice', button_style='info')\n",
    "upload_btn_2 = widgets.Button(description='Upload Speaker 2 Voice', button_style='info')\n",
    "upload_btn_3 = widgets.Button(description='Upload Speaker 3 Voice', button_style='info')\n",
    "upload_btn_4 = widgets.Button(description='Upload Speaker 4 Voice', button_style='info')\n",
    "\n",
    "# Status labels for uploads\n",
    "status_label_1 = widgets.Label(value='No file uploaded')\n",
    "status_label_2 = widgets.Label(value='No file uploaded')\n",
    "status_label_3 = widgets.Label(value='No file uploaded')\n",
    "status_label_4 = widgets.Label(value='No file uploaded')\n",
    "\n",
    "# Generation parameters\n",
    "diffusion_steps = widgets.IntSlider(value=20, min=5, max=100, step=1, description='Diffusion Steps:')\n",
    "cfg_scale = widgets.FloatSlider(value=1.3, min=0.5, max=3.5, step=0.05, description='CFG Scale:')\n",
    "seed_input = widgets.IntText(value=42, description='Seed:')\n",
    "\n",
    "# Sampling settings\n",
    "use_sampling = widgets.Checkbox(value=False, description='Use Sampling')\n",
    "temperature = widgets.FloatSlider(value=0.95, min=0.1, max=2.0, step=0.05, description='Temperature:')\n",
    "top_p = widgets.FloatSlider(value=0.95, min=0.1, max=1.0, step=0.05, description='Top P:')\n",
    "max_words = widgets.IntSlider(value=250, min=100, max=500, step=50, description='Max Words/Chunk:')\n",
    "\n",
    "# Generate button\n",
    "generate_btn = widgets.Button(description='Generate Speech', button_style='success', icon='play')\n",
    "\n",
    "# Progress bar\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progress:',\n",
    "    bar_style='info',\n",
    "    orientation='horizontal',\n",
    "    layout=widgets.Layout(width='95%')\n",
    ")\n",
    "\n",
    "# Status text\n",
    "status_text = widgets.Label(value='Ready')\n",
    "\n",
    "# Upload handlers\n",
    "def upload_audio(speaker_num):\n",
    "    def handler(b):\n",
    "        uploaded = files.upload()\n",
    "        if uploaded:\n",
    "            filename = list(uploaded.keys())[0]\n",
    "            # Save file temporarily\n",
    "            with open(f'/tmp/speaker_{speaker_num}.wav', 'wb') as f:\n",
    "                f.write(uploaded[filename])\n",
    "            uploaded_audio_files[speaker_num] = f'/tmp/speaker_{speaker_num}.wav'\n",
    "            \n",
    "            # Update status label\n",
    "            status_labels = {1: status_label_1, 2: status_label_2, 3: status_label_3, 4: status_label_4}\n",
    "            status_labels[speaker_num].value = f'✓ {filename}'\n",
    "    return handler\n",
    "\n",
    "upload_btn_1.on_click(upload_audio(1))\n",
    "upload_btn_2.on_click(upload_audio(2))\n",
    "upload_btn_3.on_click(upload_audio(3))\n",
    "upload_btn_4.on_click(upload_audio(4))\n",
    "\n",
    "# Generate speech handler\n",
    "def on_generate_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        try:\n",
    "            progress_bar.value = 0\n",
    "            status_text.value = 'Preparing...'\n",
    "            \n",
    "            text = text_input.value\n",
    "            if not text.strip():\n",
    "                print(\"❌ Error: Please enter some text\")\n",
    "                return\n",
    "            \n",
    "            # Set seeds\n",
    "            seed_val = seed_input.value\n",
    "            torch.manual_seed(seed_val)\n",
    "            torch.cuda.manual_seed(seed_val)\n",
    "            torch.cuda.manual_seed_all(seed_val)\n",
    "            np.random.seed(seed_val)\n",
    "            \n",
    "            # Set diffusion steps\n",
    "            model.set_ddpm_inference_steps(diffusion_steps.value)\n",
    "            \n",
    "            # Load uploaded voices\n",
    "            uploaded_voices = {}\n",
    "            for speaker_num, file_path in uploaded_audio_files.items():\n",
    "                if file_path is not None and os.path.exists(file_path):\n",
    "                    uploaded_voices[speaker_num] = load_audio_file(file_path)\n",
    "            \n",
    "            # Determine mode\n",
    "            active_speakers = list(uploaded_voices.keys())\n",
    "            if len(active_speakers) <= 1:\n",
    "                mode = \"Single Speaker\"\n",
    "            else:\n",
    "                mode = \"Multi Speaker\"\n",
    "            \n",
    "            progress_bar.value = 10\n",
    "            status_text.value = f'Mode: {mode}'\n",
    "            print(f\"🎤 Mode: {mode}\")\n",
    "            \n",
    "            # Parse text for pauses\n",
    "            segments = parse_pause_keywords(text)\n",
    "            all_audio_segments = []\n",
    "            sample_rate = 24000\n",
    "            \n",
    "            if mode == 'Single Speaker':\n",
    "                voice_samples = None\n",
    "                total_segments = len([s for s in segments if s[0] == 'text'])\n",
    "                current_segment = 0\n",
    "                \n",
    "                for seg_type, seg_content in segments:\n",
    "                    if seg_type == 'pause':\n",
    "                        silence = generate_silence(seg_content, sample_rate)\n",
    "                        all_audio_segments.append(silence)\n",
    "                    else:\n",
    "                        word_count = len(seg_content.split())\n",
    "                        \n",
    "                        if word_count > max_words.value:\n",
    "                            chunks = split_text_into_chunks(seg_content, max_words.value)\n",
    "                            \n",
    "                            for chunk_idx, chunk in enumerate(chunks):\n",
    "                                progress_val = 10 + int(80 * (current_segment + chunk_idx/len(chunks)) / total_segments)\n",
    "                                progress_bar.value = progress_val\n",
    "                                status_text.value = f'Generating chunk {chunk_idx+1}/{len(chunks)}'\n",
    "                                \n",
    "                                formatted_text = f\"Speaker 1: {chunk}\"\n",
    "                                \n",
    "                                if voice_samples is None:\n",
    "                                    if 1 in uploaded_voices:\n",
    "                                        voice_samples = [uploaded_voices[1]]\n",
    "                                    else:\n",
    "                                        voice_samples = [create_synthetic_voice_sample(0)]\n",
    "                                \n",
    "                                chunk_audio = generate_audio_segment(\n",
    "                                    formatted_text, voice_samples, cfg_scale.value,\n",
    "                                    seed_val, use_sampling.value, temperature.value, top_p.value\n",
    "                                )\n",
    "                                all_audio_segments.append(chunk_audio)\n",
    "                        else:\n",
    "                            progress_val = 10 + int(80 * current_segment / total_segments)\n",
    "                            progress_bar.value = progress_val\n",
    "                            status_text.value = f'Generating speech ({word_count} words)'\n",
    "                            \n",
    "                            formatted_text = f\"Speaker 1: {seg_content}\"\n",
    "                            \n",
    "                            if voice_samples is None:\n",
    "                                if 1 in uploaded_voices:\n",
    "                                    voice_samples = [uploaded_voices[1]]\n",
    "                                else:\n",
    "                                    voice_samples = [create_synthetic_voice_sample(0)]\n",
    "                            \n",
    "                            segment_audio = generate_audio_segment(\n",
    "                                formatted_text, voice_samples, cfg_scale.value,\n",
    "                                seed_val, use_sampling.value, temperature.value, top_p.value\n",
    "                            )\n",
    "                            all_audio_segments.append(segment_audio)\n",
    "                        \n",
    "                        current_segment += 1\n",
    "            else:\n",
    "                # Multi speaker mode\n",
    "                progress_bar.value = 20\n",
    "                status_text.value = 'Processing multi-speaker text'\n",
    "                \n",
    "                bracket_pattern = r'\\[(\\d+)\\]\\s*:'\n",
    "                speaker_numbers = sorted(list(set([int(m) for m in re.findall(bracket_pattern, text)])))\n",
    "                \n",
    "                if not speaker_numbers:\n",
    "                    speaker_numbers = [1]\n",
    "                \n",
    "                # Prepare voice samples\n",
    "                voice_samples = []\n",
    "                for speaker_num in speaker_numbers:\n",
    "                    if speaker_num in uploaded_voices:\n",
    "                        voice_samples.append(uploaded_voices[speaker_num])\n",
    "                    else:\n",
    "                        voice_samples.append(create_synthetic_voice_sample(speaker_num - 1))\n",
    "                \n",
    "                # Convert [N]: format to Speaker (N-1): format\n",
    "                converted_text = text\n",
    "                for speaker_num in sorted(speaker_numbers, reverse=True):\n",
    "                    pattern = f'\\\\[{speaker_num}\\\\]\\\\s*:'\n",
    "                    replacement = f'Speaker {speaker_num - 1}:'\n",
    "                    converted_text = re.sub(pattern, replacement, converted_text)\n",
    "                \n",
    "                converted_text = converted_text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "                converted_text = ' '.join(converted_text.split())\n",
    "                \n",
    "                progress_bar.value = 50\n",
    "                status_text.value = f'Generating multi-speaker speech ({len(speaker_numbers)} speakers)'\n",
    "                \n",
    "                audio = generate_audio_segment(\n",
    "                    converted_text, voice_samples, cfg_scale.value,\n",
    "                    seed_val, use_sampling.value, temperature.value, top_p.value\n",
    "                )\n",
    "                all_audio_segments.append(audio)\n",
    "            \n",
    "            # Concatenate all audio segments\n",
    "            progress_bar.value = 90\n",
    "            status_text.value = 'Finalizing audio'\n",
    "            \n",
    "            if all_audio_segments:\n",
    "                final_audio = np.concatenate(all_audio_segments)\n",
    "                \n",
    "                # Save to file\n",
    "                output_path = \"/tmp/vibevoice_output.wav\"\n",
    "                sf.write(output_path, final_audio, sample_rate)\n",
    "                \n",
    "                progress_bar.value = 100\n",
    "                status_text.value = 'Complete!'\n",
    "                \n",
    "                duration = len(final_audio) / sample_rate\n",
    "                print(f\"\\n✅ Generation Complete!\")\n",
    "                print(f\"Mode: {mode}\")\n",
    "                print(f\"Duration: {duration:.2f}s\")\n",
    "                print(f\"Sample Rate: {sample_rate} Hz\")\n",
    "                print(f\"\\n🔊 Playing audio...\")\n",
    "                \n",
    "                # Display audio player\n",
    "                display(Audio(final_audio, rate=sample_rate))\n",
    "            else:\n",
    "                print(\"❌ Error: No audio segments generated\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"❌ Error: {str(e)}\")\n",
    "            print(traceback.format_exc())\n",
    "            status_text.value = 'Error occurred'\n",
    "\n",
    "generate_btn.on_click(on_generate_clicked)\n",
    "\n",
    "# Layout\n",
    "print(\"Creating interface...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a9248",
   "metadata": {},
   "source": [
    "## 8. Display Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d8446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the interface\n",
    "display(HTML('<h2>🎙️ VibeVoice Text-to-Speech Interface</h2>'))\n",
    "display(HTML('<p>Upload voice samples for up to 4 speakers and generate speech with voice cloning.</p>'))\n",
    "\n",
    "display(HTML('<h3>Voice Samples</h3>'))\n",
    "display(widgets.HBox([upload_btn_1, status_label_1]))\n",
    "display(widgets.HBox([upload_btn_2, status_label_2]))\n",
    "display(widgets.HBox([upload_btn_3, status_label_3]))\n",
    "display(widgets.HBox([upload_btn_4, status_label_4]))\n",
    "\n",
    "display(HTML('<h3>Text Input</h3>'))\n",
    "display(text_input)\n",
    "\n",
    "display(HTML('<h3>Generation Parameters</h3>'))\n",
    "display(widgets.VBox([\n",
    "    diffusion_steps,\n",
    "    cfg_scale,\n",
    "    seed_input,\n",
    "    widgets.HTML('<b>Sampling Settings</b>'),\n",
    "    use_sampling,\n",
    "    temperature,\n",
    "    top_p,\n",
    "    max_words\n",
    "]))\n",
    "\n",
    "display(HTML('<h3>Generate</h3>'))\n",
    "display(generate_btn)\n",
    "display(progress_bar)\n",
    "display(status_text)\n",
    "\n",
    "display(HTML('<h3>Output</h3>'))\n",
    "display(output_area)\n",
    "\n",
    "display(HTML('''\n",
    "<h3>Examples</h3>\n",
    "<p><b>Single Speaker with pause:</b></p>\n",
    "<pre>Hello, this is a test. [pause:500] Let me continue speaking. [pause] And here's the final part.</pre>\n",
    "\n",
    "<p><b>Multi Speaker:</b></p>\n",
    "<pre>[1]: Hello, how are you today?\n",
    "[2]: I'm doing great, thanks for asking!\n",
    "[1]: That's wonderful to hear.</pre>\n",
    "'''))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
